{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQ2RHKXxmlek"
   },
   "source": [
    "# Realtime Medical Mask Detection Based on Faster RCNN\n",
    "Based on `Faster R-CNN`, we train model on our mask dataset and leverage data augmentation to preprocess our data. Mean average precision is introduced to evaluate the model performance, and we compare between models with and without data augmentation. In the last part, we embed our detection model into camera on computer to achieve real-time detection.\n",
    "\n",
    "This notebook is a tutorial guidance of our project. We have detailed explaination on `data augmentation`, `training` and `mAP evaluation` process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QBtmaLcVD6df"
   },
   "outputs": [],
   "source": [
    "#Load Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vR0_ut_Yn9Y5"
   },
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a2D0WheCbh_l"
   },
   "outputs": [],
   "source": [
    "!pip install pascal_voc_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_TRa2FM2JZby"
   },
   "outputs": [],
   "source": [
    "!python ../data_augmentation/augment.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NdYaHvrLL-C"
   },
   "source": [
    "## Initialization & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "id": "-M5Kerv0OOVt"
   },
   "outputs": [],
   "source": [
    "# This project is developed from the Faster RCNN turtorial Notebook on: https://www.kaggle.com/daniel601/pytorch-fasterrcnn\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# For example, here's several helpful packages to load\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from bs4 import BeautifulSoup\n",
    "import xml.etree.ElementTree as ET\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms, datasets, models\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "#Config settings\n",
    "data_path = r'../data_augmentation/output'\n",
    "output_path = r'../results'\n",
    "model_path = output_path+r'/trained_models'\n",
    "\n",
    "\n",
    "#----------------following functions are used to read annotations---------------------\n",
    "def generate_box(obj):\n",
    "  xmin = int(obj.find('xmin').text)\n",
    "  ymin = int(obj.find('ymin').text)\n",
    "  xmax = int(obj.find('xmax').text)\n",
    "  ymax = int(obj.find('ymax').text)\n",
    "\n",
    "  return [xmin, ymin, xmax, ymax]\n",
    "\n",
    "#transfer class name into numbers\n",
    "#We have three classes in mask dataset: with_mask/without_mask/mask_weared_incorrect\n",
    "def generate_label(obj):\n",
    "  #Start from one, since FastRCNNPredictor considers label 0 as background.\n",
    "  if obj.find('name').text == \"with_mask\":\n",
    "    return 1\n",
    "  elif obj.find('name').text == \"without_mask\":\n",
    "    return 2\n",
    "  elif obj.find('name').text == \"mask_weared_incorrect\":\n",
    "    return 3 \n",
    "  return None\n",
    "\n",
    "def generate_target(image_id, file): \n",
    "  with open(file) as f:\n",
    "    data = f.read()\n",
    "    soup = BeautifulSoup(data, 'xml')\n",
    "    objects = soup.find_all('object')\n",
    "\n",
    "    num_objs = len(objects)\n",
    "\n",
    "    # Bounding boxes for objects\n",
    "    # In coco format, bbox = [xmin, ymin, width, height]\n",
    "    # In pytorch, the input should be [xmin, ymin, xmax, ymax]\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    for i in objects:\n",
    "        boxes.append(generate_box(i))\n",
    "        labels.append(generate_label(i))\n",
    "    boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "    # Labels\n",
    "    labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "    # Tensorise img_id\n",
    "    img_id = torch.tensor([image_id])\n",
    "    # To transfer Annotation into dictionary format\n",
    "    target = {}\n",
    "    target[\"boxes\"] = boxes\n",
    "    target[\"labels\"] = labels\n",
    "    target[\"image_id\"] = img_id\n",
    "  return target\n",
    "#----------------functions above are used to read annotations---------------------\n",
    "\n",
    "#Generate the sample list and split datasets into trainset and testset\n",
    "allimgs_list = list(sorted(os.listdir(data_path+\"/images\")))\n",
    "alllabels_list = list(sorted(os.listdir(data_path+\"/annotations\")))\n",
    "#test_size represents the percentage of test set data in all images\n",
    "#i.e. test_size = 0.25 means training_set : test_set = 3 : 1\n",
    "trainset_imgs, testset_imgs, trainset_labels, testset_labels = train_test_split(allimgs_list, alllabels_list, test_size=0.25)\n",
    "\n",
    "class MaskDataset(object):\n",
    "    def __init__(self, transforms, set_imgs, set_labels):\n",
    "      self.transforms = transforms\n",
    "      # load all image files, sorting them to\n",
    "      # ensure that they are aligned\n",
    "      self.imgs, self.labels = set_imgs, set_labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "      # load images ad masks\n",
    "      file_image = self.imgs[idx]\n",
    "      file_label = self.labels[idx]\n",
    "      img_path = os.path.join(data_path+\"/images\", file_image)\n",
    "      label_path = os.path.join(data_path+\"/annotations\", file_label)\n",
    "      img = Image.open(img_path).convert(\"RGB\")\n",
    "      #Generate Label\n",
    "      target = generate_target(idx, label_path)\n",
    "      \n",
    "      if self.transforms is not None:\n",
    "        img = self.transforms(img)\n",
    "\n",
    "      return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "      return len(self.imgs)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "#Define transformer, more adumentations can be added into the compose function\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "trainset = MaskDataset(data_transform, trainset_imgs, trainset_labels)\n",
    "testset = MaskDataset(data_transform, testset_imgs, testset_labels)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(trainset, batch_size=4, collate_fn=collate_fn)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "print('training set:', len(data_loader)*4)\n",
    "print('test size:', len(test_loader))\n",
    "#Check whether GPU is available\n",
    "torch.cuda.is_available()\n",
    "\n",
    "#Read labels in test annotations mainly used for mAP computation\n",
    "def parse_rec(filename):\n",
    "  with open(filename) as f:\n",
    "    objects = []\n",
    "    data = f.read()\n",
    "    soup = BeautifulSoup(data, 'xml')\n",
    "    objs = soup.find_all('object')\n",
    "    num_objs = len(objs)\n",
    "    for obj in objs:\n",
    "      obj_struct = {}\n",
    "      obj_struct['name'] = obj.find('name').text\n",
    "      objects.append(obj_struct)\n",
    "      obj_struct['pose'] = obj.find('pose').text\n",
    "      obj_struct['truncated'] = int(obj.find('truncated').text)\n",
    "      obj_struct['difficult'] = int(obj.find('difficult').text)\n",
    "      bbox = obj.find('bndbox')\n",
    "      obj_struct['bbox'] = [int(bbox.find('xmin').text),\n",
    "                  int(bbox.find('ymin').text),\n",
    "                  int(bbox.find('xmax').text),\n",
    "                  int(bbox.find('ymax').text)]\n",
    "  return objects\n",
    "\n",
    "#change the program path\n",
    "if not os.path.exists(output_path):\n",
    "  os.makedirs(output_path)\n",
    "os.chdir(output_path)\n",
    "#clear out original contents\n",
    "f = open('imagesetfile.txt','w')\n",
    "f.truncate()\n",
    "f.close()\n",
    "\n",
    "#Record testset label data to get prepared for mAP evaluation\n",
    "for ann_file in testset.labels:\n",
    "  f = open('imagesetfile.txt','a')\n",
    "  f.write(ann_file.split('.')[0]+'\\n')\n",
    "  f.close()\n",
    "# read list of images\n",
    "annopath = data_path+ r'/annotations/{}.xml'\n",
    "f = open('imagesetfile.txt', 'r')\n",
    "lines = f.readlines()\n",
    "recs = {}\n",
    "for i, imagename in enumerate(lines):\n",
    "  imagename = imagename.strip('\\n')\n",
    "  recs[imagename] = parse_rec(annopath.format(imagename))\n",
    "  if i % 100 == 0: #progress bar\n",
    "    print( 'Reading annotation for {:d}/{:d}'.format(i,len(lines)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCB2wYwyOOV6"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0a1M2x8LOOV7"
   },
   "outputs": [],
   "source": [
    "def get_model_instance_segmentation(num_classes): \n",
    "    # load an instance segmentation model pre-trained pre-trained on COCO \n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True) \n",
    "    # get number of input features for the classifier \n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features \n",
    "    # replace the pre-trained head with a new one \n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes+1) \n",
    "    # plus background \n",
    "    return model\n",
    "\n",
    "model = get_model_instance_segmentation(3)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tyk2dHOtGcse"
   },
   "source": [
    "## mAP Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qdRBK6v2GcS_"
   },
   "outputs": [],
   "source": [
    "#Here we define two methods to compute AP value\n",
    "#Thanks for mAP tutorial on \n",
    "def voc_ap(rec, prec, use_07_metric=False):\n",
    "    \"\"\" ap = voc_ap(rec, prec, [use_07_metric])\n",
    "    Compute VOC AP given precision and recall.\n",
    "    If use_07_metric is true, uses the\n",
    "    VOC 07 11 point method (default:False).\n",
    "    \"\"\"\n",
    "    if use_07_metric:\n",
    "        # 11 point metric\n",
    "        ap = 0.\n",
    "        for t in np.arange(0., 1.1, 0.1):\n",
    "            if np.sum(rec >= t) == 0:\n",
    "                p = 0\n",
    "            else:\n",
    "                p = np.max(prec[rec >= t])\n",
    "            ap = ap + p / 11.\n",
    "            print('use 11 point metric')\n",
    "            print('t =', t, 'ap =', ap)\n",
    "    else:\n",
    "        # correct AP calculation\n",
    "        # first append sentinel values at the end\n",
    "        mrec = np.concatenate(([0.], rec, [1.]))\n",
    "        mpre = np.concatenate(([0.], prec, [0.]))\n",
    "\n",
    "        # compute the precision envelope\n",
    "        for i in range(mpre.size - 1, 0, -1):\n",
    "            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "        # to calculate area under PR curve, look for points\n",
    "        # where X axis (recall) changes value\n",
    "        i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "\n",
    "        # and sum (\\Delta recall) * prec\n",
    "        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap\n",
    "\n",
    "#store predictions for testset\n",
    "def pred_stor(model, test_loader, testset_labels):\n",
    "  num_test = 0\n",
    "  for i in range(1,4):#clear out the origional file\n",
    "    f = open(f'class{i}.txt', 'w')\n",
    "    f.truncate()\n",
    "    f.close()\n",
    "  for imgs, annotations in test_loader:\n",
    "    imgs = list(img.to(device) for img in imgs)\n",
    "    annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "    model.eval()\n",
    "    output = model(imgs)#output=[{'boxes':tensor([],device) 'labels': 'scores': }*batchsize]\n",
    "    for pre_dic in output:\n",
    "      filename = testset_labels[num_test].split('.')[0]\n",
    "      if filename.count('m') == 2:#sometimes the file name amy accidentally saved as 'mmaksssksksss' with number\n",
    "        filename = filename[1:]#(why it happens remained unclear)\n",
    "        print('!!!!!!!!!!!!!!!!!!!!!double m occurred!!!!!!!!!!!!!!!!!!!!!')\n",
    "      for i, cla in enumerate(pre_dic['labels']):\n",
    "        f_class = open(\"class{}.txt\".format(cla),'a')\n",
    "        box = pre_dic['boxes'][i]\n",
    "        f_class.write(\"{} {} {} {} {} {}\\n\".format(filename, pre_dic['scores'][i].item(),box[0],box[1],box[2],box[3]))\n",
    "        f_class.close()\n",
    "      num_test += 1\n",
    "  return None\n",
    "\n",
    "\n",
    "#Main evaluation function, which will read prediction and gt values to compute Recall, Precision, AP\n",
    "def voc_eval(detpath,\n",
    "             annopath,\n",
    "             imagesetfile,\n",
    "             recs,\n",
    "             classname,\n",
    "             ovthresh=0.5,\n",
    "             use_07_metric=False):\n",
    "    \"\"\"rec, prec, ap = voc_eval(detpath,\n",
    "                                annopath,\n",
    "                                imagesetfile,\n",
    "                                recs,\n",
    "                                classname,\n",
    "                                [ovthresh],\n",
    "                                [use_07_metric])\n",
    "\n",
    "    Top level function that does the PASCAL VOC evaluation.\n",
    "\n",
    "    detpath: Path to detections\n",
    "        detpath.format(classname) path to .txt files of classes' predictions\n",
    "    annopath: Path to annotations\n",
    "        annopath.format(imagename) path to .xml files of test set labels\n",
    "    imagesetfile: path of imagesetfile\n",
    "    [classname]: one specific class name\n",
    "    [ovthresh]: IOU Overlap (default = 0.5)\n",
    "    [use_07_metric]: Whether to use VOC07's 11 point AP calculation(default False)\n",
    "    \"\"\"\n",
    "    # assumes detections are in detpath.format(classname)\n",
    "    # assumes annotations are in annopath.format(imagename)\n",
    "    # assumes imagesetfile is a text file with each line an image name\n",
    "\n",
    "    # first load gt\n",
    "    if classname == '1':\n",
    "      name = 'with_mask'\n",
    "    elif classname == '2':\n",
    "      name = 'without_mask'\n",
    "    elif classname == '3':\n",
    "      name = 'mask_weared_incorrect'\n",
    "\n",
    "    # read list of images\n",
    "    f = open(imagesetfile, 'r')\n",
    "    lines = f.readlines() \n",
    "    class_recs = {}  #Save Ground Truth data \n",
    "    npos = 0\n",
    "    for imagename in lines:\n",
    "      imagename = imagename.strip('\\n')\n",
    "      #Retrieves the Ground Truth in each file of a certain type of object\n",
    "      R = [obj for obj in recs[imagename] if obj['name'] == name]\n",
    "\n",
    "      bbox = np.array([x['bbox'] for x in R])\n",
    "      #Different is almost 0/False.\n",
    "      difficult = np.array([x['difficult'] for x in R]).astype(np.bool)\n",
    "      det = [False] * len(R)\n",
    "      npos = npos + sum(~difficult) # increment, ~ Difficult inverse, count the number of samples\n",
    "\n",
    "      #Record the contents of Ground Truth\n",
    "      class_recs[imagename] = {'bbox': bbox, 'difficult': difficult, 'det': det}\n",
    "\n",
    "    #read dets -- Read the predicted output of a certain class\n",
    "    detfile = detpath.format(classname)\n",
    "\n",
    "    with open(detfile, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    splitlines = [x.strip().split(' ') for x in lines]\n",
    "    image_ids = [x[0].split('.')[0] for x in splitlines]  # Image ID\n",
    "\n",
    "    confidence = np.array([float(x[1]) for x in splitlines]) # IOU\n",
    "    BB = np.array([[float(z) for z in x[2:]] for x in splitlines]) # bounding box vlaues\n",
    "\n",
    "    #The index of confidence is sorted in descending order according to the value size.\n",
    "    sorted_ind = np.argsort(-confidence) \n",
    "    sorted_scores = np.sort(-confidence)\n",
    "    BB = BB[sorted_ind, :] #Resort bboxes by possibilities from big to small\n",
    "    image_ids = [image_ids[x] for x in sorted_ind] #Resort images by possibilities from big to small\n",
    "\n",
    "    # go down dets and mark TPs and FPs\n",
    "    nd = len(image_ids) \n",
    "\n",
    "    tp = np.zeros(nd)\n",
    "    fp = np.zeros(nd)\n",
    "    for d in range(nd):\n",
    "        if image_ids[d].count('m') == 2:#in case of accidents(how it happens remained unclear)\n",
    "          image_ids[d] = image_ids[d][1:]\n",
    "        R = class_recs[image_ids[d]]  #ann\n",
    "\n",
    "        bb = BB[d, :].astype(float)\n",
    "        '''\n",
    "        #1.If the predictions are(x_min, y_min, x_max, y_max), then we don't need to change\n",
    "        #2.If the predictions are(x_center, y_center, h, w), we need to transform that into top,left,bottom,right\n",
    "        #Transform into(x_min, y_min, x_max, y_max)\n",
    "        top = int(bb[1]-bb[3]/2)\n",
    "        left = int(bb[0]-bb[2]/2)\n",
    "        bottom = int(bb[1]+bb[3]/2)\n",
    "        right = int(bb[0]+bb[2]/2)\n",
    "        bb = [left, top, right, bottom]\n",
    "        '''\n",
    "        ovmax = -np.inf  #Negative maximum\n",
    "        BBGT = R['bbox'].astype(float)\n",
    "\n",
    "        if BBGT.size > 0:\n",
    "            # compute overlaps\n",
    "            # intersection\n",
    "            ixmin = np.maximum(BBGT[:, 0], bb[0])\n",
    "            iymin = np.maximum(BBGT[:, 1], bb[1])\n",
    "            ixmax = np.minimum(BBGT[:, 2], bb[2])\n",
    "            iymax = np.minimum(BBGT[:, 3], bb[3])\n",
    "            iw = np.maximum(ixmax - ixmin + 1., 0.)\n",
    "            ih = np.maximum(iymax - iymin + 1., 0.)\n",
    "            inters = iw * ih\n",
    "\n",
    "            # union\n",
    "            uni = ((bb[2] - bb[0] + 1.) * (bb[3] - bb[1] + 1.) +\n",
    "                   (BBGT[:, 2] - BBGT[:, 0] + 1.) *\n",
    "                   (BBGT[:, 3] - BBGT[:, 1] + 1.) - inters)\n",
    "\n",
    "            overlaps = inters / uni\n",
    "            ovmax = np.max(overlaps) # Maximum overlap\n",
    "            jmax = np.argmax(overlaps) # ground truth of the maximum overlap\n",
    "        #Compute TP and FP numbers\n",
    "        if ovmax > ovthresh:\n",
    "            if not R['difficult'][jmax]:\n",
    "                #This GT has been detected\n",
    "                #Next time, if there is another detection result whose coincidence rate meets the threshold value\n",
    "                #We don't think we detect a new object\n",
    "                if not R['det'][jmax]: \n",
    "                    tp[d] = 1.\n",
    "                    R['det'][jmax] = 1 #To mark as been detected\n",
    "                else:\n",
    "                    fp[d] = 1.\n",
    "        else:\n",
    "            fp[d] = 1.\n",
    "\n",
    "    # compute precision recall\n",
    "    fp = np.cumsum(fp)\n",
    "    tp = np.cumsum(tp)\n",
    "    rec = tp / float(npos)\n",
    "\n",
    "    # avoid divide by zero in case the first detection matches a difficult\n",
    "    # ground truth\n",
    "    # np.finfo(np.float64).eps is an infinitesimal greater than 0\n",
    "    prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps) \n",
    "    ap = voc_ap(rec, prec, use_07_metric)\n",
    "\n",
    "    return rec, prec, ap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpOnYiyuOOV9"
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NRoDNoq36csg"
   },
   "outputs": [],
   "source": [
    "#Load TensorBoard to visualize training process\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir='runs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7juFkUA3OOV9"
   },
   "outputs": [],
   "source": [
    "#Set the epoch numbers\n",
    "num_epochs = 25\n",
    "\n",
    "#Load trained model(if you don't want to train from scratch)\n",
    "#model.load_state_dict(torch.load('../results/trained_models/Epoch_24_model.pt'))   \n",
    "\n",
    "# parameters\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)#Try ADAM optimizer if you can\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "len_dataloader = len(data_loader)\n",
    "writer = SummaryWriter()\n",
    "if not os.path.exists(model_path):\n",
    "  os.makedirs(model_path)\n",
    "for epoch in range(num_epochs):\n",
    "    i = 0    \n",
    "    epoch_loss = 0\n",
    "    mAP = []\n",
    "    for imgs, annotations in data_loader:\n",
    "        i += 1\n",
    "        imgs = list(img.to(device) for img in imgs)\n",
    "        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "\n",
    "\n",
    "        model.train()\n",
    "        loss_dict = model(imgs, annotations)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step() \n",
    "\n",
    "        print(f'Epoch: {epoch+1}/{num_epochs}, Iteration: {i}/{len_dataloader}, Loss: {losses}')\n",
    "        total_iter = epoch*len_dataloader + i\n",
    "        writer.add_scalar('iteration_loss',losses,total_iter)\n",
    "        epoch_loss += losses\n",
    "    torch.save(model.state_dict(),output_path+'/trained_models/Epoch_{}_model.pt'.format(epoch))\n",
    "    writer.add_scalar('epoch_loss',epoch_loss,epoch)\n",
    "    pred_stor(model, test_loader, testset_labels)#prediction results are stored in class1/2/3.txt files\n",
    "\n",
    "    #Compute AP for each class\n",
    "    for c in range(1,4):\n",
    "      class_name = str(c)\n",
    "      rec, prec, ap = voc_eval(output_path+'/class{}.txt', output_path+'/annotations/{}.xml',\n",
    "                             output_path+'/imagesetfile.txt', recs, class_name)\n",
    "      mAP.append(ap)\n",
    "    #Print mAP\n",
    "    meanap = float(sum(mAP)/len(mAP))\n",
    "    writer.add_scalar('mAP',meanap,epoch)\n",
    "    print('*************************************************')\n",
    "    print(f'Epoch: {epoch+1}/{num_epochs}, mAP: {meanap}')\n",
    "    print(f'AP_class1: {mAP[0]}, AP_class2: {mAP[1]}, AP_class3: {mAP[2]}')\n",
    "    print('*************************************************')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9j6q4hkQOOV_"
   },
   "source": [
    "## Function to plot image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DrQPufaiOOV_"
   },
   "outputs": [],
   "source": [
    "#Define plot function\n",
    "def plot_image(img_tensor, annotation, block=True):\n",
    "  fig,ax = plt.subplots(1)\n",
    "  img = img_tensor.cpu().data\n",
    "\n",
    "  # Display the image\n",
    "  ax.imshow( np.array( img.permute(1, 2, 0) ) )\n",
    "  \n",
    "  for box, label in zip( annotation[\"boxes\"], annotation[\"labels\"] ):\n",
    "    xmin, ymin, xmax, ymax = box\n",
    "    # Create a Rectangle patch\n",
    "    if label==1:\n",
    "      rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='g',facecolor='none')\n",
    "    elif label==2:\n",
    "      rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='r',facecolor='none')\n",
    "    elif label==3:\n",
    "      rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='y',facecolor='none')\n",
    "    \n",
    "    # Add the patch to the Axes\n",
    "    ax.add_patch(rect)\n",
    "    ax.axis(\"off\")\n",
    "    \n",
    "  plt.show(block=block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xVB8xiydOOV_"
   },
   "outputs": [],
   "source": [
    "count=0\n",
    "model.eval()\n",
    "for imgs, annotations in test_loader:\n",
    "  imgs = list(img.to(device) for img in imgs)\n",
    "  annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "  preds = model(imgs)\n",
    "\n",
    "  for i in range(len(imgs)):\n",
    "    print(\"Prediction\")\n",
    "    plot_image(imgs[i], preds[i])\n",
    "    print(\"Target\")\n",
    "    plot_image(imgs[i], annotations[i])\n",
    "  count += 1\n",
    "  if count == 20:#We will check 20 images in test set.\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcZXgpRuOOWA"
   },
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z21spkJjOOWA"
   },
   "outputs": [],
   "source": [
    "model2 = get_model_instance_segmentation(3)\n",
    "model2.load_state_dict(torch.load(model_path+'Epoch_24_model.pt'))#Modify by model name\n",
    "model2.eval()\n",
    "model2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7wfa4JPOOWB"
   },
   "outputs": [],
   "source": [
    "pred2 = model2(imgs)\n",
    "print(\"Predict with loaded model\")\n",
    "plot_image(imgs[0], pred2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HBQkRXJqfdIy"
   },
   "source": [
    "## Real-time Mask Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "boF7nJ-OhlJg"
   },
   "source": [
    "Since colab cannot connect to the local camera on your computer, you have to build a local environment, and run `camera.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cSyEWquemA8_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "pytorch-fasterrcnn.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
